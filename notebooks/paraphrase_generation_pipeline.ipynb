{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Generation System - Complete Pipeline\n",
    "\n",
    "This notebook demonstrates the complete pipeline for the Paraphrase Generation System, including:\n",
    "\n",
    "1. **Data Cleaning & Preprocessing** - Loading and preparing PAWS dataset\n",
    "2. **Model Training** - Fine-tuning FLAN-T5 with LoRA\n",
    "3. **Paraphrase Generation** - Generating paraphrases from both CPG and LLM models\n",
    "4. **Score Comparison** - Comparing models using BLEU, ROUGE, and semantic similarity metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Core imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Cleaning & Preprocessing\n",
    "\n",
    "We use a high-quality paraphrase dataset:\n",
    "- **PAWS** (Paraphrase Adversaries from Word Scrambling): Challenging paraphrase pairs with high lexical overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Text Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Handles text preprocessing for paraphrase generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_words: int = 10, max_words: int = 400, remove_special_chars: bool = False):\n",
    "        self.min_words = min_words\n",
    "        self.max_words = max_words\n",
    "        self.remove_special_chars = remove_special_chars\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Optionally remove special characters\n",
    "        if self.remove_special_chars:\n",
    "            text = re.sub(r'[^\\w\\s.,!?;:\\'\"()-]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def count_words(self, text: str) -> int:\n",
    "        return len(text.split())\n",
    "    \n",
    "    def is_valid_length(self, text: str) -> bool:\n",
    "        word_count = self.count_words(text)\n",
    "        return self.min_words <= word_count <= self.max_words\n",
    "    \n",
    "    def truncate_text(self, text: str, max_words: Optional[int] = None) -> str:\n",
    "        max_words = max_words or self.max_words\n",
    "        words = text.split()\n",
    "        if len(words) > max_words:\n",
    "            words = words[:max_words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def prepare_for_model(self, text: str, prefix: str = \"paraphrase: \") -> str:\n",
    "        text = self.clean_text(text)\n",
    "        if self.max_words:\n",
    "            text = self.truncate_text(text)\n",
    "        return f\"{prefix}{text}\"\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(min_words=10, max_words=400)\n",
    "print(\"‚úÖ TextPreprocessor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paws_dataset(max_samples: Optional[int] = None):\n",
    "    \"\"\"Load PAWS dataset with positive paraphrase pairs only.\"\"\"\n",
    "    print(\"Loading PAWS dataset...\")\n",
    "    dataset = load_dataset(\"paws\", \"labeled_final\", split=\"train\", trust_remote_code=True)\n",
    "    # Filter for paraphrase pairs (label=1)\n",
    "    dataset = dataset.filter(lambda x: x[\"label\"] == 1)\n",
    "    if max_samples:\n",
    "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "    print(f\"  ‚úÖ Loaded {len(dataset)} positive paraphrase pairs from PAWS\")\n",
    "    return {\"source\": dataset[\"sentence1\"], \"target\": dataset[\"sentence2\"]}\n",
    "\n",
    "# Load dataset (using small sample for demo)\n",
    "MAX_SAMPLES = 1000  # Adjust for full training\n",
    "\n",
    "paws_data = load_paws_dataset(max_samples=MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_filter_data(data, preprocessor):\n",
    "    \"\"\"Clean and filter the dataset.\"\"\"\n",
    "    cleaned_sources = []\n",
    "    cleaned_targets = []\n",
    "    \n",
    "    for src, tgt in zip(data[\"source\"], data[\"target\"]):\n",
    "        # Clean texts\n",
    "        clean_src = preprocessor.clean_text(src)\n",
    "        clean_tgt = preprocessor.clean_text(tgt)\n",
    "        \n",
    "        # Filter by length\n",
    "        if len(clean_src.split()) >= 5 and len(clean_tgt.split()) >= 5:\n",
    "            cleaned_sources.append(clean_src)\n",
    "            cleaned_targets.append(clean_tgt)\n",
    "    \n",
    "    return {\"source\": cleaned_sources, \"target\": cleaned_targets}\n",
    "\n",
    "# Clean dataset\n",
    "print(\"\\nüìù Cleaning dataset...\")\n",
    "paws_clean = clean_and_filter_data(paws_data, preprocessor)\n",
    "\n",
    "print(f\"  PAWS: {len(paws_data['source'])} ‚Üí {len(paws_clean['source'])} samples\")\n",
    "\n",
    "# Use cleaned dataset\n",
    "all_sources = paws_clean[\"source\"]\n",
    "all_targets = paws_clean[\"target\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Total samples: {len(all_sources)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Explore Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample pairs\n",
    "print(\"üìã Sample Paraphrase Pairs:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"Pair {i+1}:\")\n",
    "    print(f\"  Source: {all_sources[i][:100]}...\")\n",
    "    print(f\"  Target: {all_targets[i][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Create Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "# Shuffle data\n",
    "random.seed(42)\n",
    "indices = list(range(len(all_sources)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "shuffled_sources = [all_sources[i] for i in indices]\n",
    "shuffled_targets = [all_targets[i] for i in indices]\n",
    "\n",
    "# Split: 80% train, 10% validation, 10% test\n",
    "n = len(shuffled_sources)\n",
    "train_end = int(0.8 * n)\n",
    "val_end = int(0.9 * n)\n",
    "\n",
    "train_data = Dataset.from_dict({\n",
    "    \"source\": shuffled_sources[:train_end],\n",
    "    \"target\": shuffled_targets[:train_end]\n",
    "})\n",
    "\n",
    "val_data = Dataset.from_dict({\n",
    "    \"source\": shuffled_sources[train_end:val_end],\n",
    "    \"target\": shuffled_targets[train_end:val_end]\n",
    "})\n",
    "\n",
    "test_data = Dataset.from_dict({\n",
    "    \"source\": shuffled_sources[val_end:],\n",
    "    \"target\": shuffled_targets[val_end:]\n",
    "})\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_data,\n",
    "    \"validation\": val_data,\n",
    "    \"test\": test_data\n",
    "})\n",
    "\n",
    "print(f\"üìä Dataset Splits:\")\n",
    "print(f\"  Train: {len(datasets['train'])} samples\")\n",
    "print(f\"  Validation: {len(datasets['validation'])} samples\")\n",
    "print(f\"  Test: {len(datasets['test'])} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Training\n",
    "\n",
    "We fine-tune FLAN-T5-base using LoRA (Low-Rank Adaptation) for parameter-efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initialize Base Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Apply LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                          # LoRA rank\n",
    "    lora_alpha=32,                 # LoRA alpha\n",
    "    lora_dropout=0.1,             # Dropout\n",
    "    target_modules=[\"q\", \"v\"],    # Target attention modules\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Move to device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"\\n‚úÖ Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from typing import Dict\n",
    "\n",
    "class ParaphraseDataset(TorchDataset):\n",
    "    \"\"\"PyTorch Dataset for paraphrase generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, tokenizer, max_input_length=512, max_output_length=512):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        item = self.dataset[idx]\n",
    "        source = self.preprocessor.prepare_for_model(item[\"source\"], \"paraphrase: \")\n",
    "        target = item[\"target\"]\n",
    "        \n",
    "        # Tokenize input\n",
    "        source_encoding = self.tokenizer(\n",
    "            source, max_length=self.max_input_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize target\n",
    "        target_encoding = self.tokenizer(\n",
    "            target, max_length=self.max_output_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = target_encoding[\"input_ids\"].squeeze()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": source_encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": source_encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ParaphraseDataset(datasets[\"train\"], tokenizer)\n",
    "val_dataset = ParaphraseDataset(datasets[\"validation\"], tokenizer)\n",
    "\n",
    "print(f\"‚úÖ Created PyTorch datasets\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../outputs/checkpoints\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_dir=\"../outputs/logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[],\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration set\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print()\n",
    "\n",
    "# Train (uncomment to run full training)\n",
    "# train_result = trainer.train()\n",
    "# print(\"\\n‚úÖ Training completed!\")\n",
    "\n",
    "# For demo, we'll skip training and load pretrained model\n",
    "print(\"‚ö†Ô∏è Training skipped for demo. Uncomment above to run full training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model (after training)\n",
    "SAVE_PATH = \"../outputs/checkpoints/final_model\"\n",
    "\n",
    "# Uncomment after training:\n",
    "# model.save_pretrained(SAVE_PATH)\n",
    "# tokenizer.save_pretrained(SAVE_PATH)\n",
    "# print(f\"‚úÖ Model saved to {SAVE_PATH}\")\n",
    "\n",
    "print(f\"Model would be saved to: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Paraphrase Generation\n",
    "\n",
    "Generate paraphrases using both models:\n",
    "- **CPG (Custom Paraphrase Generator)**: Fine-tuned FLAN-T5-base with LoRA\n",
    "- **LLM Baseline**: FLAN-T5-large (zero-shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, we'll use the models from the src package\n",
    "from src.models.cpg_model import CustomParaphraseGenerator\n",
    "from src.models.llm_baseline import LLMBaseline\n",
    "\n",
    "# Check if fine-tuned model exists\n",
    "FINETUNED_PATH = \"../outputs/checkpoints/final_model\"\n",
    "\n",
    "if os.path.exists(FINETUNED_PATH):\n",
    "    print(\"Loading fine-tuned CPG model...\")\n",
    "    cpg_model = CustomParaphraseGenerator.load(FINETUNED_PATH)\n",
    "else:\n",
    "    print(\"Fine-tuned model not found. Using base model with LoRA...\")\n",
    "    cpg_model = CustomParaphraseGenerator(\n",
    "        model_name=\"google/flan-t5-base\",\n",
    "        use_lora=True\n",
    "    )\n",
    "\n",
    "print(\"\\nLoading LLM Baseline (FLAN-T5-large)...\")\n",
    "llm_model = LLMBaseline(model_name=\"google/flan-t5-large\")\n",
    "\n",
    "print(\"\\n‚úÖ Both models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sample Text for Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text (cover letter excerpt)\n",
    "sample_text = \"\"\"\n",
    "A cover letter is a formal document that accompanies your resume when you apply for a job. \n",
    "It serves as an introduction and provides additional context for your application. \n",
    "The primary purpose of a cover letter is to introduce yourself to the hiring manager \n",
    "and to provide context for your resume. It allows you to elaborate on your qualifications, \n",
    "skills, and experiences in a way that your resume may not fully capture. It's also an \n",
    "opportunity to express your enthusiasm for the role and the company, and to explain why \n",
    "you would be a good fit.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Clean the sample\n",
    "sample_text = preprocessor.clean_text(sample_text)\n",
    "\n",
    "print(f\"üìÑ Sample Text ({len(sample_text.split())} words):\")\n",
    "print(\"-\" * 70)\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Generate Paraphrases - CPG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Generating paraphrase with CPG (Fine-tuned FLAN-T5)...\")\n",
    "print()\n",
    "\n",
    "# Measure latency\n",
    "start_time = time.perf_counter()\n",
    "cpg_paraphrase = cpg_model.generate(\n",
    "    sample_text,\n",
    "    max_length=512,\n",
    "    min_length_ratio=0.8,\n",
    "    num_beams=4,\n",
    "    temperature=0.7,\n",
    "    length_penalty=2.0\n",
    ")\n",
    "cpg_latency = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"üìù CPG Output ({len(cpg_paraphrase.split())} words):\")\n",
    "print(\"-\" * 70)\n",
    "print(cpg_paraphrase)\n",
    "print(f\"\\n‚è±Ô∏è Latency: {cpg_latency:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Generate Paraphrases - LLM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Generating paraphrase with LLM Baseline (FLAN-T5-large)...\")\n",
    "print()\n",
    "\n",
    "# Measure latency\n",
    "start_time = time.perf_counter()\n",
    "llm_paraphrase = llm_model.generate(\n",
    "    sample_text,\n",
    "    max_length=512,\n",
    "    num_beams=4,\n",
    "    temperature=0.7,\n",
    "    length_penalty=1.0\n",
    ")\n",
    "llm_latency = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"üìù LLM Output ({len(llm_paraphrase.split())} words):\")\n",
    "print(\"-\" * 70)\n",
    "print(llm_paraphrase)\n",
    "print(f\"\\n‚è±Ô∏è Latency: {llm_latency:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PARAPHRASE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìÑ ORIGINAL ({len(sample_text.split())} words):\")\n",
    "print(\"-\" * 80)\n",
    "print(sample_text[:300] + \"...\" if len(sample_text) > 300 else sample_text)\n",
    "\n",
    "print(f\"\\nüîµ CPG Model ({len(cpg_paraphrase.split())} words, {cpg_latency:.3f}s):\")\n",
    "print(\"-\" * 80)\n",
    "print(cpg_paraphrase[:300] + \"...\" if len(cpg_paraphrase) > 300 else cpg_paraphrase)\n",
    "\n",
    "print(f\"\\nüü¢ LLM Baseline ({len(llm_paraphrase.split())} words, {llm_latency:.3f}s):\")\n",
    "print(\"-\" * 80)\n",
    "print(llm_paraphrase[:300] + \"...\" if len(llm_paraphrase) > 300 else llm_paraphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Score Comparison & Evaluation\n",
    "\n",
    "Compare the models using multiple metrics:\n",
    "- **BLEU Score**: Measures n-gram precision\n",
    "- **ROUGE Score**: Measures recall of n-grams\n",
    "- **Semantic Similarity**: TF-IDF based cosine similarity\n",
    "- **Length Ratio**: Output length preservation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialize Metrics Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate as hf_evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"Calculate evaluation metrics for paraphrase generation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bleu_scorer = hf_evaluate.load(\"sacrebleu\")\n",
    "        self.rouge_scorer = hf_evaluate.load(\"rouge\")\n",
    "    \n",
    "    def calculate_bleu(self, predictions, references):\n",
    "        \"\"\"Calculate BLEU score.\"\"\"\n",
    "        refs = [[ref] for ref in references]\n",
    "        result = self.bleu_scorer.compute(predictions=predictions, references=refs)\n",
    "        return {\"bleu\": result[\"score\"]}\n",
    "    \n",
    "    def calculate_rouge(self, predictions, references):\n",
    "        \"\"\"Calculate ROUGE scores.\"\"\"\n",
    "        result = self.rouge_scorer.compute(predictions=predictions, references=references)\n",
    "        return {\n",
    "            \"rouge1\": result[\"rouge1\"],\n",
    "            \"rouge2\": result[\"rouge2\"],\n",
    "            \"rougeL\": result[\"rougeL\"]\n",
    "        }\n",
    "    \n",
    "    def calculate_semantic_similarity(self, predictions, references):\n",
    "        \"\"\"Calculate semantic similarity using TF-IDF.\"\"\"\n",
    "        similarities = []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            try:\n",
    "                tfidf_matrix = vectorizer.fit_transform([ref, pred])\n",
    "                sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "                similarities.append(sim)\n",
    "            except ValueError:\n",
    "                similarities.append(0.0)\n",
    "        return {\"semantic_similarity\": np.mean(similarities)}\n",
    "    \n",
    "    def calculate_length_ratio(self, predictions, references):\n",
    "        \"\"\"Calculate length preservation ratio.\"\"\"\n",
    "        ratios = []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            pred_len = len(pred.split())\n",
    "            ref_len = len(ref.split())\n",
    "            if ref_len > 0:\n",
    "                ratios.append(pred_len / ref_len)\n",
    "        return {\n",
    "            \"avg_length_ratio\": np.mean(ratios),\n",
    "            \"length_preservation_rate\": sum(1 for r in ratios if r >= 0.8) / len(ratios)\n",
    "        }\n",
    "    \n",
    "    def calculate_all(self, predictions, references):\n",
    "        \"\"\"Calculate all metrics.\"\"\"\n",
    "        results = {}\n",
    "        results.update(self.calculate_bleu(predictions, references))\n",
    "        results.update(self.calculate_rouge(predictions, references))\n",
    "        results.update(self.calculate_semantic_similarity(predictions, references))\n",
    "        results.update(self.calculate_length_ratio(predictions, references))\n",
    "        return results\n",
    "\n",
    "metrics_calc = MetricsCalculator()\n",
    "print(\"‚úÖ Metrics Calculator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Calculate Metrics for Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for single sample\n",
    "references = [sample_text]\n",
    "cpg_predictions = [cpg_paraphrase]\n",
    "llm_predictions = [llm_paraphrase]\n",
    "\n",
    "print(\"üìä Calculating metrics for CPG model...\")\n",
    "cpg_metrics = metrics_calc.calculate_all(cpg_predictions, references)\n",
    "\n",
    "print(\"üìä Calculating metrics for LLM model...\")\n",
    "llm_metrics = metrics_calc.calculate_all(llm_predictions, references)\n",
    "\n",
    "print(\"\\n‚úÖ Metrics calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Display Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    \"Metric\": [],\n",
    "    \"CPG (Fine-tuned)\": [],\n",
    "    \"LLM Baseline\": [],\n",
    "    \"Winner\": []\n",
    "}\n",
    "\n",
    "for metric in cpg_metrics.keys():\n",
    "    cpg_val = cpg_metrics[metric]\n",
    "    llm_val = llm_metrics[metric]\n",
    "    \n",
    "    comparison_data[\"Metric\"].append(metric)\n",
    "    comparison_data[\"CPG (Fine-tuned)\"].append(f\"{cpg_val:.4f}\")\n",
    "    comparison_data[\"LLM Baseline\"].append(f\"{llm_val:.4f}\")\n",
    "    \n",
    "    # Determine winner (higher is better for most metrics)\n",
    "    if metric == \"avg_length_ratio\":\n",
    "        # Closer to 1.0 is better\n",
    "        winner = \"CPG\" if abs(cpg_val - 1.0) < abs(llm_val - 1.0) else \"LLM\"\n",
    "    else:\n",
    "        winner = \"CPG\" if cpg_val > llm_val else \"LLM\" if llm_val > cpg_val else \"Tie\"\n",
    "    comparison_data[\"Winner\"].append(winner)\n",
    "\n",
    "# Add latency comparison\n",
    "comparison_data[\"Metric\"].append(\"latency (s)\")\n",
    "comparison_data[\"CPG (Fine-tuned)\"].append(f\"{cpg_latency:.4f}\")\n",
    "comparison_data[\"LLM Baseline\"].append(f\"{llm_latency:.4f}\")\n",
    "comparison_data[\"Winner\"].append(\"CPG\" if cpg_latency < llm_latency else \"LLM\")\n",
    "\n",
    "# Add speedup\n",
    "speedup = llm_latency / cpg_latency if cpg_latency > 0 else 0\n",
    "comparison_data[\"Metric\"].append(\"speedup\")\n",
    "comparison_data[\"CPG (Fine-tuned)\"].append(f\"{speedup:.2f}x\")\n",
    "comparison_data[\"LLM Baseline\"].append(\"1.00x\")\n",
    "comparison_data[\"Winner\"].append(\"CPG\" if speedup > 1 else \"LLM\")\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract numeric metrics for plotting\n",
    "plot_metrics = [\"bleu\", \"rouge1\", \"rouge2\", \"rougeL\", \"semantic_similarity\"]\n",
    "cpg_values = [cpg_metrics[m] for m in plot_metrics]\n",
    "llm_values = [llm_metrics[m] for m in plot_metrics]\n",
    "\n",
    "# Create bar chart\n",
    "x = np.arange(len(plot_metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Quality Metrics\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(x - width/2, cpg_values, width, label='CPG (Fine-tuned)', color='#2196F3')\n",
    "bars2 = ax1.bar(x + width/2, llm_values, width, label='LLM Baseline', color='#4CAF50')\n",
    "\n",
    "ax1.set_xlabel('Metric')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Text Quality Metrics Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([m.upper() for m in plot_metrics], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Latency Comparison\n",
    "ax2 = axes[1]\n",
    "latencies = [cpg_latency, llm_latency]\n",
    "bars = ax2.bar(['CPG (Fine-tuned)', 'LLM Baseline'], latencies, \n",
    "               color=['#2196F3', '#4CAF50'])\n",
    "ax2.set_ylabel('Latency (seconds)')\n",
    "ax2.set_title('Latency Comparison')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, lat in zip(bars, latencies):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{lat:.3f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization saved to outputs/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count wins\n",
    "cpg_wins = comparison_df['Winner'].value_counts().get('CPG', 0)\n",
    "llm_wins = comparison_df['Winner'].value_counts().get('LLM', 0)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"üìä Overall Results:\")\n",
    "print(f\"   CPG (Fine-tuned) wins: {cpg_wins} metrics\")\n",
    "print(f\"   LLM Baseline wins: {llm_wins} metrics\")\n",
    "print()\n",
    "print(f\"‚ö° Performance:\")\n",
    "print(f\"   CPG Latency: {cpg_latency:.3f}s\")\n",
    "print(f\"   LLM Latency: {llm_latency:.3f}s\")\n",
    "print(f\"   Speedup: {speedup:.2f}x faster\")\n",
    "print()\n",
    "print(f\"üìù Length Preservation:\")\n",
    "print(f\"   Original: {len(sample_text.split())} words\")\n",
    "print(f\"   CPG Output: {len(cpg_paraphrase.split())} words ({cpg_metrics['avg_length_ratio']:.1%} ratio)\")\n",
    "print(f\"   LLM Output: {len(llm_paraphrase.split())} words ({llm_metrics['avg_length_ratio']:.1%} ratio)\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ Pipeline Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Batch Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on multiple test samples\n",
    "NUM_TEST_SAMPLES = 10  # Adjust for more comprehensive evaluation\n",
    "\n",
    "test_sources = datasets['test']['source'][:NUM_TEST_SAMPLES]\n",
    "test_targets = datasets['test']['target'][:NUM_TEST_SAMPLES]\n",
    "\n",
    "print(f\"üìä Evaluating on {NUM_TEST_SAMPLES} test samples...\\n\")\n",
    "\n",
    "# Generate paraphrases from both models\n",
    "print(\"Generating CPG paraphrases...\")\n",
    "cpg_outputs = [cpg_model.generate(text) for text in test_sources]\n",
    "\n",
    "print(\"Generating LLM paraphrases...\")\n",
    "llm_outputs = [llm_model.generate(text) for text in test_sources]\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\nCalculating metrics...\")\n",
    "cpg_batch_metrics = metrics_calc.calculate_all(cpg_outputs, test_sources)\n",
    "llm_batch_metrics = metrics_calc.calculate_all(llm_outputs, test_sources)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"BATCH EVALUATION RESULTS ({NUM_TEST_SAMPLES} samples)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'CPG':<15} {'LLM':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for metric in cpg_batch_metrics.keys():\n",
    "    print(f\"{metric:<25} {cpg_batch_metrics[metric]:<15.4f} {llm_batch_metrics[metric]:<15.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

